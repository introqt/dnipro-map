"""
Advanced Address Extractor & Geocoder for Russian and Ukrainian.
Hybrid: Free AI model (Groq / Gemini) + offline NLP fallback.

Features:
  - AI-powered extraction via free LLM APIs (Groq Llama 3.3, Google Gemini)
  - Handles inflected street names, no-prefix addresses, implicit locations
  - Offline fallback: regex + natasha + pymorphy2
  - Free geocoding via Nominatim
  - Structured JSON output from LLM for reliable parsing
  - Batch processing with rate limiting

Install:
    pip install geopy natasha pymorphy2 pymorphy2-dicts-uk transliterate requests

Optional (for specific AI backends):
    pip install groq              # for Groq / Llama
    pip install google-genai      # for Google Gemini

Environment variables (set ONE):
    GROQ_API_KEY=gsk_...          # https://console.groq.com  (free)
    GEMINI_API_KEY=...            # https://aistudio.google.com (free)

Usage:
    from address_extractor import extract_and_geocode

    result = extract_and_geocode("Ğ—ÑƒÑÑ‚Ñ€Ñ–Ğ½ĞµĞ¼Ğ¾ÑÑŒ Ğ½Ğ° Ğ¥Ñ€ĞµÑ‰Ğ°Ñ‚Ğ¸ĞºÑƒ, 22 Ğ¾ Ñ‚Ñ€ĞµÑ‚Ñ–Ğ¹")
    print(result.to_dict())
"""

from __future__ import annotations

import json
import logging
import os
import re
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict, field
from typing import Optional

import requests
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError

# â”€â”€ Natasha (Russian NLP) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from natasha import (
    Segmenter, MorphVocab, NewsEmbedding,
    NewsMorphTagger, NewsNERTagger, Doc, AddrExtractor,
)
import pymorphy2

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Init heavy NLP resources (loaded once at import)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_segmenter = Segmenter()
_morph_vocab = MorphVocab()
_emb = NewsEmbedding()
_morph_tagger = NewsMorphTagger(_emb)
_ner_tagger = NewsNERTagger(_emb)
_addr_extractor = AddrExtractor(_morph_vocab)
_morph_ru = pymorphy2.MorphAnalyzer(lang="ru")

try:
    _morph_uk = pymorphy2.MorphAnalyzer(lang="uk")
except Exception:
    _morph_uk = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Data models
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class ParsedAddress:
    """Structured address extracted by any method."""
    street_type: str = ""       # "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚", etc. (nominative)
    street_name: str = ""       # "Ğ¥Ñ€ĞµÑ‰Ğ°Ñ‚Ğ¸Ğº", "Ğ¢Ğ²ĞµÑ€ÑĞºĞ°Ñ" (nominative)
    building: str = ""          # "22", "7Ğ"
    apartment: str = ""         # "5", "12"
    city: str = ""              # "ĞšĞ¸Ñ—Ğ²", "ĞœĞ¾ÑĞºĞ²Ğ°" (nominative)
    postal_code: str = ""       # "01001"
    raw_text: str = ""          # original extracted fragment
    confidence: float = 0.0     # 0.0 â€“ 1.0

    def to_geocode_string(self) -> str:
        """Build a string optimized for Nominatim geocoding."""
        parts = []
        if self.street_type and self.street_name:
            parts.append(f"{self.street_type} {self.street_name}")
        elif self.street_name:
            parts.append(self.street_name)
        if self.building:
            parts.append(self.building)
        if self.city:
            parts.append(self.city)
        return ", ".join(parts)

    def to_display_string(self) -> str:
        """Human-readable normalized address."""
        parts = []
        st = f"{self.street_type} {self.street_name}" if self.street_type else self.street_name
        if st.strip():
            parts.append(st.strip())
        if self.building:
            parts.append(self.building)
        if self.apartment:
            parts.append(f"ĞºĞ². {self.apartment}")
        if self.city:
            parts.append(self.city)
        if self.postal_code:
            parts.append(self.postal_code)
        return ", ".join(parts)


@dataclass
class GeoResult:
    """Final result of extraction + geocoding."""
    original_text: str
    parsed: Optional[ParsedAddress] = None
    latitude: Optional[float] = None
    longitude: Optional[float] = None
    display_name: Optional[str] = None
    language: str = ""
    method: str = ""            # "groq", "gemini", "regex", "natasha", "heuristic"
    geocoded: bool = False
    error: Optional[str] = None

    def to_dict(self) -> dict:
        d = asdict(self)
        return d


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Language detection
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_UK_UNIQUE = set("Ñ–Ñ—Ñ”Ò‘Ğ†Ğ‡Ğ„Ò")
_RU_UNIQUE = set("Ñ‘ÑŠÑ‹ÑĞĞªĞ«Ğ­")


def detect_language(text: str) -> str:
    chars = set(text)
    has_uk = bool(chars & _UK_UNIQUE)
    has_ru = bool(chars & _RU_UNIQUE)
    if has_uk and not has_ru:
        return "uk"
    if has_ru and not has_uk:
        return "ru"
    if has_uk and has_ru:
        return "uk"
    if re.search(r"\b(Ğ²ÑƒĞ»|Ğ¿Ñ€Ğ¾ÑĞ¿|Ğ¿Ñ€Ğ¾Ğ²|Ğ±ÑƒĞ´|Ğ¼\.)\b", text, re.IGNORECASE):
        return "uk"
    if re.search(r"\b(ÑƒĞ»|Ğ¿Ñ€-Ñ‚|Ğ¿ĞµÑ€|Ğ´\.)\b", text, re.IGNORECASE):
        return "ru"
    if re.search(r"[Ğ°-ÑĞ-Ğ¯]", text):
        return "ru"
    return "unknown"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI Backends (abstract + implementations)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ Shared prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_SYSTEM_PROMPT = """\
You are an expert address extraction system for Russian and Ukrainian texts.

TASK: Extract the physical address from the user's message.

CRITICAL RULES:
1. Convert ALL words to NOMINATIVE case (Ğ½Ğ°Ğ·Ğ¸Ğ²Ğ½Ğ¸Ğ¹ Ğ²Ñ–Ğ´Ğ¼Ñ–Ğ½Ğ¾Ğº / Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ´ĞµĞ¶):
   - "Ğ½Ğ° Ğ¥Ñ€ĞµÑ‰Ğ°Ñ‚Ğ¸ĞºÑƒ" â†’ street_name: "Ğ¥Ñ€ĞµÑ‰Ğ°Ñ‚Ğ¸Ğº"
   - "Ğ¿Ğ¾ Ğ¢Ğ²ĞµÑ€ÑĞºĞ¾Ğ¹"  â†’ street_name: "Ğ¢Ğ²ĞµÑ€ÑĞºĞ°Ñ"
   - "Ğ¨ĞµĞ²Ñ‡ĞµĞ½ĞºĞ°"     â†’ street_name: "Ğ¨ĞµĞ²Ñ‡ĞµĞ½ĞºĞ¾" (if it's a person's name used as street)
   - "Ğ½Ğ° ĞÑ€Ğ±Ğ°Ñ‚Ğµ"    â†’ street_name: "ĞÑ€Ğ±Ğ°Ñ‚"
   - "Ğ‘Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¡Ğ°Ğ´Ğ¾Ğ²ÑƒÑ" â†’ street_name: "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¡Ğ°Ğ´Ğ¾Ğ²Ğ°Ñ"
   - "Ğ½Ğ° ĞĞµĞ²ÑĞºĞ¾Ğ¼"   â†’ street_name: "ĞĞµĞ²ÑĞºĞ¸Ğ¹"
   - "Ğ“Ñ€ÑƒÑˆĞµĞ²ÑÑŒĞºĞ¾Ğ³Ğ¾"  â†’ street_name: "Ğ“Ñ€ÑƒÑˆĞµĞ²ÑÑŒĞºĞ¸Ğ¹"
   - "Ğ¢Ğ°Ñ€Ğ°ÑĞ° Ğ¨ĞµĞ²Ñ‡ĞµĞ½ĞºĞ°" â†’ street_name: "Ğ¢Ğ°Ñ€Ğ°Ñ Ğ¨ĞµĞ²Ñ‡ĞµĞ½ĞºĞ¾"

2. Detect the street type even if abbreviated or absent:
   - "Ğ²ÑƒĞ»." / "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ" / "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ–" â†’ street_type: "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ"
   - "ÑƒĞ»." / "ÑƒĞ»Ğ¸Ñ†Ğ°" / "ÑƒĞ»Ğ¸Ñ†Ğµ"    â†’ street_type: "ÑƒĞ»Ğ¸Ñ†Ğ°"
   - "Ğ¿Ñ€Ğ¾ÑĞ¿." / "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚"        â†’ street_type: "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚"
   - "Ğ¿Ñ€Ğ¾Ğ²." / "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»Ğ¾Ğº"         â†’ street_type: "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»Ğ¾Ğº"
   - "Ğ¿ĞµÑ€." / "Ğ¿ĞµÑ€ĞµÑƒĞ»Ğ¾Ğº"          â†’ street_type: "Ğ¿ĞµÑ€ĞµÑƒĞ»Ğ¾Ğº"
   - "Ğ¿Ğ»." / "Ğ¿Ğ»Ğ¾Ñ‰Ğ°" / "Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ"  â†’ street_type: "Ğ¿Ğ»Ğ¾Ñ‰Ğ°" (uk) or "Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ" (ru)
   - "Ğ±ÑƒĞ»ÑŒĞ²." / "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€"         â†’ street_type: "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€"
   - "Ğ½Ğ°Ğ±." / "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°/Ğ°Ñ"      â†’ street_type: "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°" (uk) or "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°Ñ" (ru)
   - "ÑƒĞ·Ğ²Ñ–Ğ·"                      â†’ street_type: "ÑƒĞ·Ğ²Ñ–Ğ·"
   - "ÑˆĞ¾ÑĞµ" / "ÑˆĞ¾ÑÑĞµ"             â†’ keep as is
   - If no type given, infer "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ" (uk) or "ÑƒĞ»Ğ¸Ñ†Ğ°" (ru) as default.

1. Detect the city from context (it may always be Dnipro if not mentioned).:
   - "Ñƒ Ğ”Ğ½Ñ–Ğ¿Ñ€Ñ–" / "Ğ² Ğ”Ğ½Ñ–Ğ¿Ñ€Ñ–" â†’ city: "Ğ”Ğ½Ñ–Ğ¿Ñ€Ğ¾"

4. Extract building number, apartment, postal code if present.

5. If NO address is found, return all fields as empty strings.

Respond with ONLY a JSON object, no markdown, no backticks, no explanation:
{
  "street_type": "...",
  "street_name": "...",
  "building": "...",
  "apartment": "...",
  "city": "...",
  "postal_code": "...",
  "raw_text": "...",
  "confidence": 0.0
}

"raw_text" = the substring of the original message that contains the address.
"confidence" = 0.0 to 1.0, how confident you are that an address was found.
"""


def _parse_llm_response(text: str) -> Optional[ParsedAddress]:
    """Safely parse LLM JSON response into ParsedAddress."""
    text = text.strip()
    # Strip markdown fences if present
    text = re.sub(r"^```(?:json)?\s*", "", text)
    text = re.sub(r"\s*```$", "", text)
    text = text.strip()

    try:
        data = json.loads(text)
    except json.JSONDecodeError:
        # Try to extract JSON object from the response
        m = re.search(r"\{[^{}]+\}", text, re.DOTALL)
        if m:
            try:
                data = json.loads(m.group(0))
            except json.JSONDecodeError:
                logger.warning("Failed to parse LLM response as JSON: %s", text[:200])
                return None
        else:
            return None

    # Validate: must have at least street_name or raw_text
    if not data.get("street_name") and not data.get("raw_text"):
        return None

    confidence = float(data.get("confidence", 0.0))
    if confidence < 0.3:
        return None

    return ParsedAddress(
        street_type=data.get("street_type", "").strip(),
        street_name=data.get("street_name", "").strip(),
        building=str(data.get("building", "")).strip(),
        apartment=str(data.get("apartment", "")).strip(),
        city=data.get("city", "").strip(),
        postal_code=str(data.get("postal_code", "")).strip(),
        raw_text=data.get("raw_text", "").strip(),
        confidence=confidence,
    )


# â”€â”€ Backend: Groq (Llama 3.3 70B â€” free) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class GroqBackend:
    """
    Free AI backend using Groq's API with Llama 3.3 70B.
    Get a free key at: https://console.groq.com
    Free tier: ~6,000 requests/day, 30 RPM.
    """

    API_URL = "https://api.groq.com/openai/v1/chat/completions"
    MODEL = "llama-3.3-70b-versatile"

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.environ.get("GROQ_API_KEY", "")
        self.name = "groq"

    @property
    def available(self) -> bool:
        return bool(self.api_key)

    def extract(self, text: str) -> Optional[ParsedAddress]:
        if not self.available:
            return None

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": self.MODEL,
            "messages": [
                {"role": "system", "content": _SYSTEM_PROMPT},
                {"role": "user", "content": text},
            ],
            "temperature": 0.0,
            "max_tokens": 300,
            "response_format": {"type": "json_object"},
        }

        try:
            resp = requests.post(self.API_URL, json=payload, headers=headers, timeout=15)
            resp.raise_for_status()
            content = resp.json()["choices"][0]["message"]["content"]
            return _parse_llm_response(content)
        except Exception as e:
            logger.warning("Groq API error: %s", e)
            return None


# â”€â”€ Backend: Google Gemini (free tier) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class GeminiBackend:
    """
    Free AI backend using Google Gemini API.
    Get a free key at: https://aistudio.google.com/apikey
    Free tier: 15 RPM, 1M tokens/min, 1500 req/day.
    """

    API_URL = "https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    MODEL = "gemini-2.0-flash"

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.environ.get("GEMINI_API_KEY", "")
        self.name = "gemini"

    @property
    def available(self) -> bool:
        return bool(self.api_key)

    def extract(self, text: str) -> Optional[ParsedAddress]:
        if not self.available:
            return None

        url = self.API_URL.format(model=self.MODEL) + f"?key={self.api_key}"
        payload = {
            "contents": [
                {
                    "parts": [
                        {"text": _SYSTEM_PROMPT + "\n\nUser message:\n" + text}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": 0.0,
                "maxOutputTokens": 300,
                "responseMimeType": "application/json",
            },
        }

        try:
            resp = requests.post(url, json=payload, timeout=15)
            resp.raise_for_status()
            content = resp.json()["candidates"][0]["content"]["parts"][0]["text"]
            return _parse_llm_response(content)
        except Exception as e:
            logger.warning("Gemini API error: %s", e)
            return None


# â”€â”€ Backend: Generic OpenAI-compatible (Ollama, LM Studio, etc.) â”€

class OpenAICompatibleBackend:
    """
    For any local or free OpenAI-compatible API:
      - Ollama:    base_url="http://localhost:11434/v1", model="llama3"
      - LM Studio: base_url="http://localhost:1234/v1", model="local-model"
      - Together:  base_url="https://api.together.xyz/v1", model="..."
    """

    def __init__(
        self,
        base_url: str = "http://localhost:11434/v1",
        model: str = "llama3",
        api_key: str = "",
        name: str = "openai_compat",
    ):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.api_key = api_key or os.environ.get("OPENAI_COMPAT_API_KEY", "")
        self.name = name

    @property
    def available(self) -> bool:
        # Try a quick health check
        try:
            resp = requests.get(f"{self.base_url}/models", timeout=3,
                                headers={"Authorization": f"Bearer {self.api_key}"} if self.api_key else {})
            return resp.status_code == 200
        except Exception:
            return False

    def extract(self, text: str) -> Optional[ParsedAddress]:
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"

        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": _SYSTEM_PROMPT},
                {"role": "user", "content": text},
            ],
            "temperature": 0.0,
            "max_tokens": 300,
        }

        try:
            resp = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload, headers=headers, timeout=30,
            )
            resp.raise_for_status()
            content = resp.json()["choices"][0]["message"]["content"]
            return _parse_llm_response(content)
        except Exception as e:
            logger.warning("%s API error: %s", self.name, e)
            return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Offline extraction (regex + natasha + pymorphy2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ Street types (all case forms) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_STREET_TYPES_ALL_UK = {
    "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ", "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ–", "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ", "Ğ²ÑƒĞ»Ğ¸Ñ†ĞµÑ", "Ğ²ÑƒĞ»Ğ¸Ñ†ÑŒ",
    "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ñƒ", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ¼", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ñ–",
    "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»Ğ¾Ğº", "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»ĞºÑƒ", "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»ĞºĞ¾Ğ¼", "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»ĞºÑ–",
    "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€", "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€Ñƒ", "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€Ğ¾Ğ¼", "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€Ñ–",
    "Ğ¿Ğ»Ğ¾Ñ‰Ğ°", "Ğ¿Ğ»Ğ¾Ñ‰Ñ–", "Ğ¿Ğ»Ğ¾Ñ‰Ñƒ", "Ğ¿Ğ»Ğ¾Ñ‰ĞµÑ",
    "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ¾Ñ—", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ñƒ", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ñ–Ğ¹",
    "ÑˆĞ¾ÑĞµ", "Ğ°Ğ»ĞµÑ", "Ğ°Ğ»ĞµÑ—", "Ğ°Ğ»ĞµÑ”Ñ", "Ğ°Ğ»ĞµÑ",
    "ÑƒĞ·Ğ²Ñ–Ğ·", "ÑƒĞ·Ğ²Ğ¾Ğ·Ñƒ", "ÑƒĞ·Ğ²Ğ¾Ğ·Ñ–", "ÑƒĞ·Ğ²Ğ¾Ğ·Ğ¾Ğ¼",
}
_STREET_TYPES_ALL_RU = {
    "ÑƒĞ»Ğ¸Ñ†Ğ°", "ÑƒĞ»Ğ¸Ñ†Ñ‹", "ÑƒĞ»Ğ¸Ñ†Ğµ", "ÑƒĞ»Ğ¸Ñ†Ñƒ", "ÑƒĞ»Ğ¸Ñ†ĞµĞ¹",
    "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ°", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğµ", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ñƒ", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ¼",
    "Ğ¿ĞµÑ€ĞµÑƒĞ»Ğ¾Ğº", "Ğ¿ĞµÑ€ĞµÑƒĞ»ĞºĞ°", "Ğ¿ĞµÑ€ĞµÑƒĞ»ĞºĞµ", "Ğ¿ĞµÑ€ĞµÑƒĞ»ĞºÑƒ", "Ğ¿ĞµÑ€ĞµÑƒĞ»ĞºĞ¾Ğ¼",
    "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€", "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€Ğ°", "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€Ğµ", "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€Ñƒ", "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€Ğ¾Ğ¼",
    "Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ", "Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸", "Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒÑ",
    "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°Ñ", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ¾Ğ¹", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½ÑƒÑ", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ñ‹Ñ…",
    "ÑˆĞ¾ÑÑĞµ", "Ğ°Ğ»Ğ»ĞµÑ", "Ğ°Ğ»Ğ»ĞµĞ¸", "Ğ°Ğ»Ğ»ĞµĞµ", "Ğ°Ğ»Ğ»ĞµÑ", "Ğ°Ğ»Ğ»ĞµĞµĞ¹",
    "Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´", "Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´Ğ°", "Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´Ğµ", "Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´Ñƒ", "Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´Ğ¾Ğ¼",
}
_STREET_TYPES_NOM_UK = {
    "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ", "Ğ²ÑƒĞ»", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚", "Ğ¿Ñ€Ğ¾ÑĞ¿", "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»Ğ¾Ğº", "Ğ¿Ñ€Ğ¾Ğ²",
    "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€", "Ğ±ÑƒĞ»ÑŒĞ²", "Ğ¿Ğ»Ğ¾Ñ‰Ğ°", "Ğ¿Ğ»", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°", "Ğ½Ğ°Ğ±",
    "ÑˆĞ¾ÑĞµ", "Ğ°Ğ»ĞµÑ", "ÑƒĞ·Ğ²Ñ–Ğ·",
}
_STREET_TYPES_NOM_RU = {
    "ÑƒĞ»Ğ¸Ñ†Ğ°", "ÑƒĞ»", "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚", "Ğ¿Ñ€", "Ğ¿ĞµÑ€ĞµÑƒĞ»Ğ¾Ğº", "Ğ¿ĞµÑ€",
    "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€", "Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ", "Ğ¿Ğ»", "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°Ñ", "Ğ½Ğ°Ğ±",
    "ÑˆĞ¾ÑÑĞµ", "Ğ°Ğ»Ğ»ĞµÑ", "Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´",
}
_ALL_STREET_TYPES = (
    _STREET_TYPES_ALL_UK | _STREET_TYPES_ALL_RU
    | _STREET_TYPES_NOM_UK | _STREET_TYPES_NOM_RU
    | {s + "." for s in _STREET_TYPES_NOM_UK | _STREET_TYPES_NOM_RU}
)

_STREET_TYPE_TO_NOM: dict[str, str] = {}
for _form in _STREET_TYPES_ALL_UK:
    if _form.startswith("Ğ²ÑƒĞ»Ğ¸Ñ†"): _STREET_TYPE_TO_NOM[_form] = "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ"
    elif _form.startswith("Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚"): _STREET_TYPE_TO_NOM[_form] = "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚"
    elif _form.startswith("Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»"): _STREET_TYPE_TO_NOM[_form] = "Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»Ğ¾Ğº"
    elif _form.startswith("Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€"): _STREET_TYPE_TO_NOM[_form] = "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€"
    elif _form.startswith("Ğ¿Ğ»Ğ¾Ñ‰"): _STREET_TYPE_TO_NOM[_form] = "Ğ¿Ğ»Ğ¾Ñ‰Ğ°"
    elif _form.startswith("Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½"): _STREET_TYPE_TO_NOM[_form] = "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°"
    elif _form.startswith("Ğ°Ğ»Ğµ"): _STREET_TYPE_TO_NOM[_form] = "Ğ°Ğ»ĞµÑ"
    elif _form.startswith("ÑƒĞ·Ğ²Ğ¾"): _STREET_TYPE_TO_NOM[_form] = "ÑƒĞ·Ğ²Ñ–Ğ·"
for _form in _STREET_TYPES_ALL_RU:
    if _form.startswith("ÑƒĞ»Ğ¸Ñ†"): _STREET_TYPE_TO_NOM[_form] = "ÑƒĞ»Ğ¸Ñ†Ğ°"
    elif _form.startswith("Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚"): _STREET_TYPE_TO_NOM[_form] = "Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚"
    elif _form.startswith("Ğ¿ĞµÑ€ĞµÑƒĞ»"): _STREET_TYPE_TO_NOM[_form] = "Ğ¿ĞµÑ€ĞµÑƒĞ»Ğ¾Ğº"
    elif _form.startswith("Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€"): _STREET_TYPE_TO_NOM[_form] = "Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€"
    elif _form.startswith("Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´"): _STREET_TYPE_TO_NOM[_form] = "Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ"
    elif _form.startswith("Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½"): _STREET_TYPE_TO_NOM[_form] = "Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°Ñ"
    elif _form.startswith("Ğ°Ğ»Ğ»Ğµ"): _STREET_TYPE_TO_NOM[_form] = "Ğ°Ğ»Ğ»ĞµÑ"
    elif _form.startswith("Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´"): _STREET_TYPE_TO_NOM[_form] = "Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´"


# â”€â”€ Cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_CITIES_UK = {
    "ĞšĞ¸Ñ—Ğ²": ("ĞšĞ¸Ñ—Ğ²","ĞšĞ¸Ñ”Ğ²Ñ–","ĞšĞ¸Ñ”Ğ²Ğ°","ĞšĞ¸Ñ”Ğ²Ğ¾Ğ¼"),
    "Ğ›ÑŒĞ²Ñ–Ğ²": ("Ğ›ÑŒĞ²Ñ–Ğ²","Ğ›ÑŒĞ²Ğ¾Ğ²Ñ–","Ğ›ÑŒĞ²Ğ¾Ğ²Ğ°","Ğ›ÑŒĞ²Ğ¾Ğ²Ğ¾Ğ¼"),
    "ĞĞ´ĞµÑĞ°": ("ĞĞ´ĞµÑĞ°","ĞĞ´ĞµÑÑ–","ĞĞ´ĞµÑĞ¸","ĞĞ´ĞµÑÑƒ","ĞĞ´ĞµÑĞ¾Ñ"),
    "Ğ¥Ğ°Ñ€ĞºÑ–Ğ²": ("Ğ¥Ğ°Ñ€ĞºÑ–Ğ²","Ğ¥Ğ°Ñ€ĞºĞ¾Ğ²Ñ–","Ğ¥Ğ°Ñ€ĞºĞ¾Ğ²Ğ°","Ğ¥Ğ°Ñ€ĞºĞ¾Ğ²Ğ¾Ğ¼"),
    "Ğ”Ğ½Ñ–Ğ¿Ñ€Ğ¾": ("Ğ”Ğ½Ñ–Ğ¿Ñ€Ğ¾","Ğ”Ğ½Ñ–Ğ¿Ñ€Ñ–","Ğ”Ğ½Ñ–Ğ¿Ñ€Ğ°","Ğ”Ğ½Ñ–Ğ¿Ñ€Ğ¾Ğ¼"),
    "Ğ—Ğ°Ğ¿Ğ¾Ñ€Ñ–Ğ¶Ğ¶Ñ": ("Ğ—Ğ°Ğ¿Ğ¾Ñ€Ñ–Ğ¶Ğ¶Ñ","Ğ—Ğ°Ğ¿Ğ¾Ñ€Ñ–Ğ¶Ğ¶Ñ–"),
    "Ğ’Ñ–Ğ½Ğ½Ğ¸Ñ†Ñ": ("Ğ’Ñ–Ğ½Ğ½Ğ¸Ñ†Ñ","Ğ’Ñ–Ğ½Ğ½Ğ¸Ñ†Ñ–","Ğ’Ñ–Ğ½Ğ½Ğ¸Ñ†Ñ","Ğ’Ñ–Ğ½Ğ½Ğ¸Ñ†ĞµÑ"),
    "ĞŸĞ¾Ğ»Ñ‚Ğ°Ğ²Ğ°": ("ĞŸĞ¾Ğ»Ñ‚Ğ°Ğ²Ğ°","ĞŸĞ¾Ğ»Ñ‚Ğ°Ğ²Ñ–","ĞŸĞ¾Ğ»Ñ‚Ğ°Ğ²Ğ¸","ĞŸĞ¾Ğ»Ñ‚Ğ°Ğ²Ñƒ"),
    "Ğ§ĞµÑ€Ğ½Ñ–Ğ³Ñ–Ğ²": ("Ğ§ĞµÑ€Ğ½Ñ–Ğ³Ñ–Ğ²","Ğ§ĞµÑ€Ğ½Ñ–Ğ³Ğ¾Ğ²Ñ–","Ğ§ĞµÑ€Ğ½Ñ–Ğ³Ğ¾Ğ²Ğ°"),
    "Ğ§ĞµÑ€ĞºĞ°ÑĞ¸": ("Ğ§ĞµÑ€ĞºĞ°ÑĞ¸","Ğ§ĞµÑ€ĞºĞ°ÑĞ°Ñ…","Ğ§ĞµÑ€ĞºĞ°Ñ"),
    "Ğ¡ÑƒĞ¼Ğ¸": ("Ğ¡ÑƒĞ¼Ğ¸","Ğ¡ÑƒĞ¼Ğ°Ñ…","Ğ¡ÑƒĞ¼"),
    "Ğ Ñ–Ğ²Ğ½Ğµ": ("Ğ Ñ–Ğ²Ğ½Ğµ","Ğ Ñ–Ğ²Ğ½Ğ¾Ğ¼Ñƒ"),
    "Ğ¢ĞµÑ€Ğ½Ğ¾Ğ¿Ñ–Ğ»ÑŒ": ("Ğ¢ĞµÑ€Ğ½Ğ¾Ğ¿Ñ–Ğ»ÑŒ","Ğ¢ĞµÑ€Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ñ–","Ğ¢ĞµÑ€Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ñ"),
    "Ğ›ÑƒÑ†ÑŒĞº": ("Ğ›ÑƒÑ†ÑŒĞº","Ğ›ÑƒÑ†ÑŒĞºÑƒ","Ğ›ÑƒÑ†ÑŒĞºĞ°"),
    "Ğ£Ğ¶Ğ³Ğ¾Ñ€Ğ¾Ğ´": ("Ğ£Ğ¶Ğ³Ğ¾Ñ€Ğ¾Ğ´","Ğ£Ğ¶Ğ³Ğ¾Ñ€Ğ¾Ğ´Ñ–","Ğ£Ğ¶Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°"),
    "ĞœĞ¸ĞºĞ¾Ğ»Ğ°Ñ—Ğ²": ("ĞœĞ¸ĞºĞ¾Ğ»Ğ°Ñ—Ğ²","ĞœĞ¸ĞºĞ¾Ğ»Ğ°Ñ”Ğ²Ñ–","ĞœĞ¸ĞºĞ¾Ğ»Ğ°Ñ”Ğ²Ğ°"),
    "Ğ¥Ğ¼ĞµĞ»ÑŒĞ½Ğ¸Ñ†ÑŒĞºĞ¸Ğ¹": ("Ğ¥Ğ¼ĞµĞ»ÑŒĞ½Ğ¸Ñ†ÑŒĞºĞ¸Ğ¹","Ğ¥Ğ¼ĞµĞ»ÑŒĞ½Ğ¸Ñ†ÑŒĞºĞ¾Ğ¼Ñƒ","Ğ¥Ğ¼ĞµĞ»ÑŒĞ½Ğ¸Ñ†ÑŒĞºĞ¾Ğ³Ğ¾"),
    "Ğ†Ğ²Ğ°Ğ½Ğ¾-Ğ¤Ñ€Ğ°Ğ½ĞºÑ–Ğ²ÑÑŒĞº": ("Ğ†Ğ²Ğ°Ğ½Ğ¾-Ğ¤Ñ€Ğ°Ğ½ĞºÑ–Ğ²ÑÑŒĞº","Ğ†Ğ²Ğ°Ğ½Ğ¾-Ğ¤Ñ€Ğ°Ğ½ĞºÑ–Ğ²ÑÑŒĞºÑƒ"),
    "ĞšÑ€Ğ¾Ğ¿Ğ¸Ğ²Ğ½Ğ¸Ñ†ÑŒĞºĞ¸Ğ¹": ("ĞšÑ€Ğ¾Ğ¿Ğ¸Ğ²Ğ½Ğ¸Ñ†ÑŒĞºĞ¸Ğ¹","ĞšÑ€Ğ¾Ğ¿Ğ¸Ğ²Ğ½Ğ¸Ñ†ÑŒĞºĞ¾Ğ¼Ñƒ"),
    "Ğ–Ğ¸Ñ‚Ğ¾Ğ¼Ğ¸Ñ€": ("Ğ–Ğ¸Ñ‚Ğ¾Ğ¼Ğ¸Ñ€","Ğ–Ğ¸Ñ‚Ğ¾Ğ¼Ğ¸Ñ€Ñ–","Ğ–Ğ¸Ñ‚Ğ¾Ğ¼Ğ¸Ñ€Ğ°"),
}


_CITY_LOOKUP: dict[str, str] = {}
for _nom, _forms in {**_CITIES_UK}.items():
    for _f in _forms:
        _CITY_LOOKUP[_f.lower()] = _nom


def _find_city(text: str) -> Optional[str]:
    text_lower = text.lower()
    for form in sorted(_CITY_LOOKUP.keys(), key=len, reverse=True):
        pat = r"(?:^|[\s,;.(])" + re.escape(form) + r"(?:[\s,;.!?)]|$)"
        if re.search(pat, text_lower):
            return _CITY_LOOKUP[form]
    return None


# â”€â”€ Morph helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_morph(lang: str):
    if lang == "uk" and _morph_uk:
        return _morph_uk
    return _morph_ru


def _inflect_nominative(phrase: str, lang: str) -> str:
    morph = _get_morph(lang)
    result = []
    for word in phrase.split():
        if len(word) <= 2 or word.isdigit():
            result.append(word)
            continue
        parsed = morph.parse(word)
        if not parsed:
            result.append(word)
            continue
        nom = parsed[0].inflect({"nomn"})
        if nom:
            w = nom.word
            if word[0].isupper():
                w = w[0].upper() + w[1:]
            result.append(w)
        else:
            nf = parsed[0].normal_form
            if word[0].isupper():
                nf = nf[0].upper() + nf[1:]
            result.append(nf)
    return " ".join(result)


def _normalize_street_type(word: str) -> Optional[str]:
    w = word.lower().rstrip(".")
    return _STREET_TYPE_TO_NOM.get(w) or (w if w in _STREET_TYPES_NOM_UK | _STREET_TYPES_NOM_RU else None)


# â”€â”€ Regex extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_CYR = r"[Ğ-Ğ¯Ğ†Ğ‡Ğ„ÒĞĞ°-ÑÑ–Ñ—Ñ”Ò‘Ñ‘'Ê¼\u2019\-]+"
_NUM = r"\d{1,4}\s*[Ğ-Ğ¯Ğ°-ÑA-Za-z]?"
_BLDG = r"(?:\s*,?\s*(?:Ğ±ÑƒĞ´\.?|Ğ´\.?)\s*\d+\w?)?"
_APT = r"(?:\s*,?\s*(?:ĞºĞ²\.?|Ğ¾Ñ„\.?|ĞºĞ¾Ñ€Ğ¿\.?)\s*\d+\w?)*"
_POSTAL = r"(?:\s*,?\s*\d{5,6})?"

_PREPS = (
    r"(?:Ğ½Ğ°|Ğ¿Ğ¾|Ğ±Ñ–Ğ»Ñ|ĞºĞ¾Ğ»Ğ¾|Ğ¿Ğ¾Ğ±Ğ»Ğ¸Ğ·Ñƒ|Ğ½Ğ°Ğ²Ğ¿Ñ€Ğ¾Ñ‚Ğ¸|"
    r"Ğ²Ğ¾Ğ·Ğ»Ğµ|Ğ¾ĞºĞ¾Ğ»Ğ¾|Ğ½Ğ°Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²|Ñ€ÑĞ´Ğ¾Ğ¼\s+Ñ|"
    r"Ğ·Ğ° Ğ°Ğ´Ñ€ĞµÑĞ¾Ñ|Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ)"
)

def _build_patterns() -> list[re.Pattern]:
    types_sorted = sorted(_ALL_STREET_TYPES, key=len, reverse=True)
    types_re = "|".join(re.escape(t) for t in types_sorted)
    city_names = sorted(_CITY_LOOKUP.keys(), key=len, reverse=True)
    city_re = "|".join(re.escape(c) for c in city_names)

    return [
        # P1: [prep] street_type name, number
        re.compile(
            rf"(?:{_PREPS}\s+)?(?:{types_re})\.?\s+"
            rf"({_CYR}(?:\s+{_CYR}){{0,3}})\s*,?\s*({_NUM})"
            rf"{_BLDG}{_APT}{_POSTAL}", re.I | re.U),
        # P2: name street_type, number (postfix)
        re.compile(
            rf"({_CYR}(?:\s+{_CYR}){{0,2}})\s+(?:{types_re})\.?"
            rf"\s*,?\s*({_NUM}){_BLDG}{_APT}{_POSTAL}", re.I | re.U),
        # P3: prep + name, number (no street type)
        re.compile(
            rf"{_PREPS}\s+({_CYR}(?:\s+{_CYR}){{0,3}})\s*,\s*({_NUM})"
            rf"{_BLDG}{_APT}", re.I | re.U),
        # P4: prep + name + Ğ´Ğ¾Ğ¼/Ğ±ÑƒĞ´ number
        re.compile(
            rf"{_PREPS}\s+({_CYR}(?:\s+{_CYR}){{0,3}})"
            rf"\s*,?\s*(?:Ğ´Ğ¾Ğ¼|Ğ´\.|Ğ±ÑƒĞ´\.|Ğ±ÑƒĞ´)\s*({_NUM})", re.I | re.U),
        # P5: city, [type] name, number
        re.compile(
            rf"(?:Ğ¼\.?\s*|Ğ³\.?\s*)?(?:{city_re})\s*,\s*"
            rf"(?:(?:{types_re})\.?\s+)?({_CYR}(?:\s+{_CYR}){{0,3}})"
            rf"\s*,?\s*({_NUM}){_BLDG}{_APT}{_POSTAL}", re.I | re.U),
        # P6: "address:" + freeform
        re.compile(
            r"(?:Ğ·Ğ° Ğ°Ğ´Ñ€ĞµÑĞ¾Ñ|Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ|Ğ°Ğ´Ñ€ĞµÑĞ°|Ğ°Ğ´Ñ€ĞµÑ)\s*[:\-]?\s*"
            r"([Ğ-Ğ¯Ğ†Ğ‡Ğ„ÒĞĞ°-ÑÑ–Ñ—Ñ”Ò‘Ñ‘0-9\s,.\-/'Ê¼]{8,90})", re.I | re.U),
        # P7: bare Name, number
        re.compile(
            rf"([Ğ-Ğ¯Ğ†Ğ‡Ğ„ÒĞ]{_CYR[1:]}(?:\s+{_CYR}){{0,2}})\s*,\s*({_NUM})"
            rf"{_BLDG}{_APT}", re.U),
    ]

_PATTERNS = _build_patterns()

_STOP_WORDS = {
    "Ğ°Ğ»Ğµ","Ğ°Ğ±Ğ¾","Ğ²Ñ–Ğ´","Ğ´Ğ»Ñ","Ğ¿Ñ€Ğ¸","Ğ±ĞµĞ·","Ğ¼Ñ–Ğ¶","Ğ°Ğ±Ğ¾","Ğ¸Ğ»Ğ¸","Ğ´Ğ»Ñ",
    "Ğ¿Ñ€Ğ¸","Ğ±ĞµĞ·","Ğ¼ĞµĞ¶Ğ´Ñƒ","ÑÑ‚Ğ¾","ÑÑŒĞ¾Ğ³Ğ¾Ğ´Ğ½Ñ–","Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°","Ğ²Ñ‡Ğ¾Ñ€Ğ°","Ğ·Ğ°Ñ€Ğ°Ğ·",
    "Ğ¿Ğ¾Ñ‚Ñ–Ğ¼","ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ","Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°","Ğ²Ñ‡ĞµÑ€Ğ°","ÑĞµĞ¹Ñ‡Ğ°Ñ","Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼","Ğ´Ğ¾Ğ±Ñ€Ğ¸Ğ¹",
    "Ğ´Ğ¾Ğ±Ñ€Ñ‹Ğ¹","Ğ¿Ñ€Ğ¸Ğ²Ñ–Ñ‚","Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚","Ğ´ÑĞºÑƒÑ","ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾","Ğ±ÑƒĞ´ÑŒ","Ğ»Ğ°ÑĞºĞ°",
}


def _extract_offline(text: str, lang: str) -> Optional[ParsedAddress]:
    """Regex + natasha + pymorphy2 fallback extraction."""

    city = _find_city(text)

    # â”€â”€ Try regex patterns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for idx, pat in enumerate(_PATTERNS):
        m = pat.search(text)
        if not m:
            continue

        if idx in (0, 1):
            name = m.group(1).strip()
            number = m.group(2).strip() if m.lastindex >= 2 else ""
            st_type = None
            for w in m.group(0).split():
                n = _normalize_street_type(w)
                if n:
                    st_type = n
                    break
            name_nom = _inflect_nominative(name, lang)
            if not st_type:
                st_type = "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ" if lang == "uk" else "ÑƒĞ»Ğ¸Ñ†Ğ°"
            return ParsedAddress(
                street_type=st_type, street_name=name_nom,
                building=number, city=city or "",
                raw_text=m.group(0).strip(), confidence=0.8,
            )

        elif idx in (2, 3):
            name = m.group(1).strip()
            if name.lower() in _STOP_WORDS:
                continue
            number = m.group(2).strip() if m.lastindex >= 2 else ""
            name_nom = _inflect_nominative(name, lang)
            default_type = "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ" if lang == "uk" else "ÑƒĞ»Ğ¸Ñ†Ğ°"
            return ParsedAddress(
                street_type=default_type, street_name=name_nom,
                building=number, city=city or "",
                raw_text=m.group(0).strip(), confidence=0.7,
            )

        elif idx == 4:
            name = m.group(1).strip()
            number = m.group(2).strip() if m.lastindex >= 2 else ""
            name_nom = _inflect_nominative(name, lang)
            default_type = "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ" if lang == "uk" else "ÑƒĞ»Ğ¸Ñ†Ğ°"
            return ParsedAddress(
                street_type=default_type, street_name=name_nom,
                building=number, city=city or "",
                raw_text=m.group(0).strip(), confidence=0.75,
            )

        elif idx == 5:
            raw = m.group(1).strip().rstrip(",;.!")
            return ParsedAddress(
                raw_text=raw, confidence=0.6,
                city=city or "",
            )

        elif idx == 6:
            name = m.group(1).strip()
            if name.lower() in _STOP_WORDS:
                continue
            if not any(len(w) >= 4 and w[0].isupper() for w in name.split()):
                continue
            number = m.group(2).strip() if m.lastindex >= 2 else ""
            name_nom = _inflect_nominative(name, lang)
            default_type = "Ğ²ÑƒĞ»Ğ¸Ñ†Ñ" if lang == "uk" else "ÑƒĞ»Ğ¸Ñ†Ğ°"
            return ParsedAddress(
                street_type=default_type, street_name=name_nom,
                building=number, city=city or "",
                raw_text=m.group(0).strip(), confidence=0.5,
            )

    # â”€â”€ Natasha fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    matches = list(_addr_extractor(text))
    if matches:
        start = min(m_.start for m_ in matches)
        stop = max(m_.stop for m_ in matches)
        raw = text[start:stop].strip().rstrip(",;.!")
        if len(raw) > 5:
            after = text[stop:stop + 20]
            nm = re.match(r"\s*,?\s*(\d{1,4}\w?)", after)
            number = nm.group(1) if nm else ""
            return ParsedAddress(
                raw_text=raw, building=number,
                city=city or "", confidence=0.6,
            )

    # â”€â”€ NER fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    doc = Doc(text)
    doc.segment(_segmenter)
    doc.tag_morph(_morph_tagger)
    doc.tag_ner(_ner_tagger)
    locs = [s for s in doc.spans if s.type == "LOC"]
    if locs:
        best = max(locs, key=lambda s: s.stop - s.start)
        raw = text[best.start:best.stop].strip()
        after = text[best.stop:best.stop + 20]
        nm = re.match(r"\s*,?\s*(\d{1,4}\w?)", after)
        number = nm.group(1) if nm else ""
        return ParsedAddress(
            raw_text=raw, building=number,
            city=city or "", confidence=0.5,
        )

    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Geocoding
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_geolocator = Nominatim(user_agent="addr_extract_ai_v3/1.0", timeout=10)


def _geocode(address: str, lang: str = "uk", retries: int = 2) -> Optional[dict]:
    """Geocode with multiple fallback query strategies."""
    attempts = [address]

    # Expand abbreviations
    repls = {
        "Ğ²ÑƒĞ».":"Ğ²ÑƒĞ»Ğ¸Ñ†Ñ","Ğ¿Ñ€Ğ¾ÑĞ¿.":"Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚","Ğ¿Ñ€Ğ¾Ğ².":"Ğ¿Ñ€Ğ¾Ğ²ÑƒĞ»Ğ¾Ğº",
        "Ğ±ÑƒĞ»ÑŒĞ².":"Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€","Ğ¿Ğ».":"Ğ¿Ğ»Ğ¾Ñ‰Ğ°","Ğ½Ğ°Ğ±.":"Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°",
        "Ğ±ÑƒĞ´.":"","Ğ¼.":"","ÑƒĞ».":"ÑƒĞ»Ğ¸Ñ†Ğ°","Ğ¿Ñ€-Ñ‚":"Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚",
        "Ğ¿ĞµÑ€.":"Ğ¿ĞµÑ€ĞµÑƒĞ»Ğ¾Ğº","Ğ´.":"","Ğ³.":"",
    }
    cleaned = address
    for s, f in repls.items():
        cleaned = cleaned.replace(s, f)
    cleaned = re.sub(r",?\s*(?:ĞºĞ²\.?|Ğ¾Ñ„\.?|ĞºĞ¾Ñ€Ğ¿\.?)\s*\d+\w?", "", cleaned, flags=re.I)
    cleaned = cleaned.strip().rstrip(",")
    if cleaned != address:
        attempts.append(cleaned)

    try:
        from transliterate import translit
        attempts.append(translit(address, reversed=True))
    except ImportError:
        pass

    simplified = re.sub(r",\s*\d{1,4}\w?\b", "", address).strip().rstrip(",")
    if simplified != address:
        attempts.append(simplified)

    seen = set()
    unique = [a for a in attempts if a.strip() and a.strip() not in seen and not seen.add(a.strip())]

    for attempt in unique:
        for _ in range(retries):
            try:
                loc = _geolocator.geocode(attempt, language=lang, addressdetails=True)
                if loc:
                    return {
                        "latitude": loc.latitude,
                        "longitude": loc.longitude,
                        "display_name": loc.raw.get("display_name", ""),
                        "address_details": loc.raw.get("address", {}),
                        "query_used": attempt,
                    }
                break
            except GeocoderTimedOut:
                time.sleep(1)
            except GeocoderServiceError:
                return None
        time.sleep(1.1)

    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Orchestrator: choose backend, extract, geocode
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Default backends (ordered by preference)
_backends: list = []


def configure(
    groq_api_key: Optional[str] = None,
    gemini_api_key: Optional[str] = None,
    ollama_url: Optional[str] = None,
    ollama_model: str = "llama3",
    custom_backends: Optional[list] = None,
):
    """
    Configure which AI backends to use (in priority order).
    Call once at startup; afterwards extract_and_geocode() uses them.

    Args:
        groq_api_key:    Groq API key (free at console.groq.com)
        gemini_api_key:  Google Gemini key (free at aistudio.google.com)
        ollama_url:      Ollama base URL (e.g. "http://localhost:11434/v1")
        ollama_model:    Ollama model name (default: "llama3")
        custom_backends: List of backend instances (GroqBackend, GeminiBackend, etc.)
    """
    global _backends
    _backends = []

    if custom_backends:
        _backends.extend(custom_backends)
        return

    # Auto-detect from args or env
    groq = GroqBackend(api_key=groq_api_key)
    if groq.available:
        _backends.append(groq)

    gemini = GeminiBackend(api_key=gemini_api_key)
    if gemini.available:
        _backends.append(gemini)

    if ollama_url:
        ollama = OpenAICompatibleBackend(
            base_url=ollama_url, model=ollama_model, name="ollama"
        )
        if ollama.available:
            _backends.append(ollama)

    if not _backends:
        logger.info(
            "No AI backend configured. Using offline extraction only. "
            "Set GROQ_API_KEY or GEMINI_API_KEY for better results."
        )


def _auto_configure():
    """Auto-configure from environment on first use."""
    if not _backends:
        configure()


def extract_and_geocode(
    text: str,
    city_hint: Optional[str] = None,
) -> GeoResult:
    """
    Extract an address from Russian/Ukrainian text and geocode it.

    Pipeline:
      1. Try each configured AI backend (Groq â†’ Gemini â†’ Ollama â†’ ...)
      2. Fall back to offline (regex â†’ natasha â†’ heuristic)
      3. Geocode the best result via Nominatim

    Args:
        text:       Input text containing an address.
        city_hint:  Optional city (nominative) to help geocoding.

    Returns:
        GeoResult with parsed address and coordinates.
    """
    _auto_configure()

    lang = detect_language(text)
    parsed: Optional[ParsedAddress] = None
    method = "none"

    # â”€â”€ Step 1: Try AI backends â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for backend in _backends:
        try:
            parsed = backend.extract(text)
            if parsed and parsed.confidence >= 0.3:
                method = backend.name
                # If AI didn't find city, try detecting from text
                if not parsed.city:
                    parsed.city = city_hint or _find_city(text) or ""
                break
            parsed = None
        except Exception as e:
            logger.warning("Backend %s failed: %s", backend.name, e)
            continue

    # â”€â”€ Step 2: Offline fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not parsed:
        parsed = _extract_offline(text, lang)
        if parsed:
            method = "offline"
            if not parsed.city:
                parsed.city = city_hint or _find_city(text) or ""

    if not parsed:
        return GeoResult(
            original_text=text, language=lang,
            method="none", error="No address found",
        )

    # â”€â”€ Step 3: Geocode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Build geocoding query from parsed address
    geocode_query = parsed.to_geocode_string()
    geo = _geocode(geocode_query, lang=lang if lang != "unknown" else "en")

    # Fallback: try raw_text + city
    if not geo and parsed.raw_text:
        fallback = parsed.raw_text
        if parsed.city and parsed.city.lower() not in fallback.lower():
            fallback += f", {parsed.city}"
        geo = _geocode(fallback, lang=lang if lang != "unknown" else "en")

    result = GeoResult(
        original_text=text,
        parsed=parsed,
        language=lang,
        method=method,
    )

    if geo:
        result.latitude = geo["latitude"]
        result.longitude = geo["longitude"]
        result.display_name = geo["display_name"]
        result.geocoded = True
    else:
        result.error = "Address extracted but geocoding failed"

    return result


def extract_and_geocode_batch(
    texts: list[str],
    city_hint: Optional[str] = None,
    delay: float = 1.1,
) -> list[GeoResult]:
    """Process multiple texts with rate limiting."""
    results = []
    for text in texts:
        result = extract_and_geocode(text, city_hint=city_hint)
        results.append(result)
        if result.geocoded:
            time.sleep(delay)
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Demo
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Auto-configures from GROQ_API_KEY / GEMINI_API_KEY env vars.
    # Or configure manually:
    #   configure(groq_api_key="gsk_...")
    #   configure(gemini_api_key="AI...")
    #   configure(ollama_url="http://localhost:11434/v1", ollama_model="llama3")

    tests = [
        " Ğ¡ĞºĞ¾Ñ€Ñ–Ñˆ Ğ·Ğ° Ğ²ÑĞµ Ğ¿Ñ–Ğ´Ğ°Ñ€Ğ¸ Ğ½Ğ° Ğ±Ñ–Ğ»Ğ¾Ğ¼Ñƒ Ğ´Ğ°ÑÑ‚Ğ¾Ñ€Ñ– Ğ½Ğ° ĞœÑ–Ñ…Ğ½Ğ¾Ğ²ÑÑŒĞºĞ¾Ğ³Ğ¾ ÑÑ‚Ğ¾ÑÑ‚ÑŒ, Ğ»Ñ–Ğ²Ğ¸Ğ¹ 3",
        " ĞĞ¾Ğ²Ğ¾ĞºÑ€Ğ¸Ğ¼ÑÑŒĞºĞ°, Ğ½Ğ°Ğ²Ğ¿Ñ€Ğ¾Ñ‚Ğ¸ Ğ¡Ñ–Ğ»ÑŒĞ¿Ğ¾ Ğ¿Ñ–Ñ€Ğ¾Ğ¶Ğ¾Ğº 317-95ĞĞ• Ğ¿Ğ¸Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸",

        # â”€â”€ Ukrainian: WITH prefix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "Ğ—ÑƒÑÑ‚Ñ€Ñ–Ñ‡ Ğ·Ğ° Ğ°Ğ´Ñ€ĞµÑĞ¾Ñ Ğ²ÑƒĞ». Ğ¥Ñ€ĞµÑ‰Ğ°Ñ‚Ğ¸Ğº, 22, ĞšĞ¸Ñ—Ğ², 01001.",

        # â”€â”€ Russian: inflected, NO prefix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "  ĞŸĞ¾ ÑĞ²Ğ¾Ñ€Ğ½Ğ¸Ñ†ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ· , Ğ±ĞµĞ»Ñ‹Ğ¹ Ğ±ÑƒÑ Ñ€ĞµĞ½Ğ¾ , Ğ·Ğ° Ğ½Ğ¸Ğ¼ Ğ±ĞµĞ»Ñ‹Ğ¹ Ğ´Ğ°ÑÑ‚ĞµÑ€ , 7 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ½Ğ°Ğ·Ğ°Ğ´ Ğ±Ñ‹Ğ»Ğ¸ Ğ½Ğ° Ğ´Ğ·ĞµÑ€Ğ¶Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾",
        "  ĞŸĞ¾ Ñ…Ğ¾Ğ´Ñƒ Ğ½Ğ° Ğ¿Ñ€.Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ¾ Ğ¿Ğ°ĞºÑƒÑÑ‚,ĞºÑ€Ğ¸ĞºĞ¸ Ğ¾Ñ€,Ğ¼ÑƒĞ¶Ğ¸ĞºĞ¸ Ğ¸ Ğ±Ğ°Ğ±Ğ°,Ñ€Ğ°Ğ¹Ğ¾Ğ½ Ğ”ĞµĞ»Ğ²Ğ¸,Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ¾Ğº ĞºĞ°Ğº Ğ½Ğ° Ğ”Ğ—Ğ¼Ğ¾",
        "  ĞŸĞ¸Ğ´Ğ°Ñ€Ñ‹ Ğ½Ğ° ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ 29 Ğ²Ğ¾ Ğ´Ğ²Ğ¾Ñ€Ğ°Ñ… Ğ³Ğ½Ğ°Ğ»Ğ¸ÑÑŒ Ğ·Ğ° Ğ¿Ğ°Ñ€Ğ½ÑĞ¼Ğ¸.",
        "  Ğ›ĞµĞ²Ñ‹Ğ¹ 2 Ğ±ĞµĞ»Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ¾ Ğ¸ ĞœĞ°Ğ·Ğ´Ğ° Ğ¿Ğ¾ Ğ´Ğ²Ğ¾Ñ€Ğ°Ğ¼",
        "  Ğ¡Ğ»Ğ¾Ğ±Ğ¾Ğ¶Ğ°Ğ½ÑĞºĞ¸Ğ¹ 70. Ğ’Ğ¾Ğ·Ğ»Ğµ ĞÑ‚Ğ± Ğ½Ğ° Ğ¾ĞºĞµĞ°Ğ½Ğµ ĞĞ° 19:30 Ğ‘ĞµĞ³Ğ°Ğ»Ğ¸ Ğ·Ğ° 1, Ğ¸ 1 Ğ¿Ğ¾Ğ¹Ğ¼Ğ°Ğ»Ğ¸",

        # â”€â”€ Russian: WITH prefix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "  Ğ¢Ğ¸Ñ‚Ğ¾Ğ²Ğ° Ğ½ĞµĞ´Ğ°Ğ»ĞµĞºĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚ĞºĞ° Ğ¿Ñ€. ĞŸĞ¾Ğ»Ñ, ÑÑ‚Ğ¾ÑÑ‚ Ğ½Ğ° Ğ°Ğ²Ğ°Ñ€Ğ¸Ğ¹ĞºĞ°Ñ… 2 Ñ‡ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°ÑÑ‚ĞµÑ€.",
    ]

    active = [b.name for b in _backends] or ["offline only"]
    print("=" * 70)
    print(f"ADDRESS EXTRACTOR v3 â€” AI-powered")
    print(f"Active backends: {', '.join(active)}")
    print("=" * 70)

    for msg in tests:
        print(f"\n{'â”€' * 60}")
        print(f"ğŸ“©  {msg}")
        r = extract_and_geocode(msg)

        if r.geocoded and r.parsed:
            print(f"  âœ… Parsed:     {r.parsed.to_display_string()}")
            print(f"     Geocode â†’   {r.parsed.to_geocode_string()}")
            print(f"     Method:     {r.method} (conf: {r.parsed.confidence:.1f})")
            print(f"     Coords:     ({r.latitude:.6f}, {r.longitude:.6f})")
        elif r.parsed:
            print(f"  âš ï¸  Parsed:     {r.parsed.to_display_string()}")
            print(f"     Error:      {r.error}")
        else:
            print(f"  âŒ {r.error}")

        time.sleep(1.2)
